// This is a first attempt at providing a method for running bits of
// optimized programs. I think we can do more/better though.

// Calling the regular function `fn` evaluates the function, sampling
// from the target. Calling the result of usingParams(fn) evaluates
// the function sampling from the guide.

var usingGuide = function(fn) {
  return function() {
    var args = arguments;
    sample(Infer({method: 'forward', samples: 1, guide: true, model() {
      return ad.valueRec(apply(fn, args));
    }}));
  };
};

var checkNetName = function(name) {
  return webpplNn.checkNetName(name);
};

var mergeObj = function(o1, o2) {
  return _.assign({}, o1, o2);
};

// ==================================================
// Model Parameters
// ==================================================

var parameterModel = function(getPrior) {
  return function(paramOpts) {
    return sample(getPrior(paramOpts), {guide() {
      return Delta({v: param(paramOpts)});
    }});
  };
};

// var modelParam = parameterModel(constF(ImproperUniform()));

// The function returned by e.g. modelParamL2(0.1) is analogous the
// WebPPL's `modelParam`.

var modelParamL2 = function(sigma) {
  return parameterModel(function(paramOpts) {
    var dims = paramOpts.dims;
    return dims ?
        TensorGaussian({mu: 0, sigma, dims}) :
        Gaussian({mu: 0, sigma});
  });
};

// ==================================================
// Neural Networks
// ==================================================

var dims = function(x) {
  return ad.value(x).dims;
};

var concat = function(arr) {
  var t = T.concat(arr);
  return T.reshape(t, [dims(t)[0], 1]);
};

var idMatrix = function(n) {
  return webpplNn.idMatrix(n);
};

var oneHot = function(index, length) {
  return webpplNn.oneHot(index, length);
};

var softplus = function(x) {
  return T.log(T.add(T.exp(x), 1));
};

var softmax = function(x) {
  return T.softmax(x);
};

var squishToProbSimplex = function(x) {
  return dists.squishToProbSimplex(x);
};

var sigmoid = function(x) {
  return T.sigmoid(x);
};

var tanh = function(x) {
  return T.tanh(x);
};

var relu = function(x) {
  return webpplNn.relu(x);
};

var lrelu = function(x) {
  return webpplNn.lrelu(x);
};

// Compose several functions in right to left order.
// stack([f, g, h]) == compose(f, compose(g, h))
var stack = function(arr) {
  return reduce(compose, idF, arr);
};

// When a network is used in the model we often want to include a
// prior over the network's parameters. (So that we're specifying
// fully probabilistic model that could in principle be used with any
// inference algorithm.)

// In order that neural network helpers can be reused in both the
// model and the guide, we need to abstract out the specification of
// the prior and its guide distribution.

// This is done using the model parameter scheme described above.
// Specifically, the neural network constructors take as an argument
// the function that is used to create/fetch parameters. If this
// function is `param` then we end up with a net suitable for use in
// the guide. Alternatively a parameter model function can be passed
// to create a net suitable for use in the model.

// TODO: Write about the problems that arise in taking this approach.

// The biggest problem I see with this approach is inherited from
// `param`. I'll describe the problem with that, the problem with nets
// is similar. A parameter is created/fetched with something like
// `param({name, dims, init})`. Note, we can induce parameter sharing
// by reusing a name more than once. The problem is that when sharing
// is across two or more distinct locations in source code, then there
// is no longer a single canonical location in which dims (and other
// properties) are specified. (The dims from the `param` call
// encountered first will be used.) At best this is an annoyance
// (because the easiest way to cope is to use the same dims at every
// occurrence of a name), but I suspect that in practice it will be
// worse than annoying. A bug arising as a result of a tensor getting
// unexpected dims by virtue of a definition somewhere else in the
// program will, I suspect, cause much confusion. (Especially if the
// sharing by name is accidental, or worse, caused by loading a
// package!) I think this problem stems from the fact that `param`
// smushes together both creation and retrieval of parameters.
// Splitting apart these two operations would offer a number of ways
// of addressing this I think.


// Nets created with "linear" default to using a weight initialization
// based on "Understanding the difficulty of training deep feedforward
// neural networks", often called Xavier initialization.

// TODO: This initialization scheme may not be suitable for asymmetric
// non-linearities.

var xavier = function(dims) {
  assert.ok(dims.length === 2, 'Xavier initialization is only defined for matrices.');
  var nin = dims[0];
  var nout = dims[1];
  var sigma = Math.sqrt(2 / (nin + nout));
  return TensorGaussian({mu: 0, sigma, dims}).sample();
};

// I switched to creating parameters when a net is constructed rather
// than when it is used, primarily for the following reason:

// If the parameters are created when the net is used, then it is not
// possible to use a net that uses model parameters from within
// Enumerate nested within Optimize. (Because this would attempt to
// sample the parameters from within Enumerate.) By creating the
// parameters in the constructor, users have the flexibility required
// to create a network within `Optimize` and then use the network
// within nested `Enumerate`.

// Further, making this change also allowed me to remove the cached
// implementation of model parameters, which existed primarily to
// support lazy parameter creation. This is probably a good thing,
// since using this caching across coroutine boundaries seems iffy.
// For example, even if it was correct, you can imagine been in a
// situation where you call modelParamL2 within Optimize and later
// within a nested Enumerate. This would have worked because of
// caching. But if the call within Optimize is removed, the call
// within the nested Enumerate will start to fail. (As it is now
// attempts to sample from a continuous distribution within
// Enumerate.) This kind of non-local effect is unpleaseant to deal
// with and best avoided.

var linear = function(name, maybeArgs) {
  checkNetName(name);
  var args = maybeArgs || {};
  var nin = args.in;
  var nout = args.out;
  assert.ok(Number.isInteger(nin), 'Argument "in" should be an integer.');
  assert.ok(Number.isInteger(nout), 'Argument "out" should be an integer.');
  var nnparam = args.param || param;
  var init = args.init || xavier;
  var w = nnparam({name, dims: [nout, nin], init});
  return function(x) {
    return T.dot(w, x);
  };
};

// TODO: It seems pretty common to not apply regularization to biases.
// Is that important? What would it look like here?

var bias = function(name, maybeArgs) {
  checkNetName(name);
  var args = maybeArgs || {};
  var nnparam = args.param || param;
  var initb = _.has(args, 'initb') ? args.initb : 0;
  assert.ok(_.isNumber(initb), 'Initial bias should be a number.');
  var nout = args.out;
  assert.ok(Number.isInteger(nout), 'Argument "out" should be an integer.');
  var b = nnparam({name, dims: [nout, 1], mu: initb, sigma: 0});
  return function(x) {
    assert.ok(dims(x)[0] === nout,
              'Input vector has unexpected dimension.');
    return T.add(x, b);
  };
};

var affine = function(name, maybeArgs) {
  checkNetName(name);
  return compose(
    bias(name + 'b', maybeArgs),
    linear(name + 'w', maybeArgs));
};
