// Center and rescale the input. Typical usage is to put this after a
// linear layer and follow it with a bias and non-linearity, yielding
// "Layer normalization".

// https://arxiv.org/abs/1607.06450

var layerNorm = function(name, maybeArgs) {
  assert.ok(name, 'A network must be given a name');
  var args = maybeArgs || {};
  var nnparam = args.param || param;
  return function(a) {
    var nout = dims(a)[0];
    // Gain parameters.
    var g = nnparam({name: name + 'g', dims: [nout, 1], mu: 1, sigma: 0});
    var mu = T.sumreduce(a) / nout;
    var acentered = T.sub(a, mu);
    var sd = Math.sqrt(T.sumreduce(T.pow(acentered, 2)) / nout);
    assert.ok(ad.value(sd) > 0, 'Standard deviation of activations is not positive');
    return T.mul(T.div(g, sd), acentered);
  };
};

// TODO: You can imagine having a convenient way to do a restricted
// kind of composition of network constructors, where we assume the
// sub nets are stacked, and each take the same nout param. (Also
// requires more consistency in the interface of current constructors,
// e.g. we'd need to do something i like have a version of bias that
// takes a dummy nout param.) This could be useful for passing novel
// constructors to gru/lstm etc.

var affineWithLayerNorm = function(name, nout, maybeArgs) {
  assert.ok(name, 'A network must be given a name');
  return stack([
    bias(name + 'b', maybeArgs),
    layerNorm(name + 'ln', maybeArgs),
    linear(name + 'w', nout, maybeArgs)
  ]);
};

// TODO: Remove the bias term from weight norm. Create
// `affineWeightNorm` with composition.

// Linear layer with weight normalization.
// https://arxiv.org/abs/1602.07868

// It's not clear how we could do the data dependent init., so we
// resort to Xavier initialization. Is this sensible? The problem is
// this might be used as part of a recurrent net (which is problematic
// for the reason mentioned in the paper) and because we don't have a
// straight-forward way of looking at a 'batch' from here.

// Does it make sense to apply regularization v here, given that we
// already normalize the length of the row vectors of v? In practice,
// regularization seems to have a much stronger effect on this
// parameterization. (You can see this with L2 prior by comparing this
// with the linear layer on the ml-classifier example.) Removing the
// prior from v seems to make a difference here.

// Note: When I tried parameterizing g in log space the ml-classifier
// example often failed to work well. I don't know why.

var linearWeightNorm = function(name, nout, maybeArgs) {
  assert.ok(name, 'A network must be given a name');
  assert.ok(Number.isInteger(nout), 'nout should be an integer');
  var args = maybeArgs || {};
  var nnparam = args.param || param;
  return function(x) {
    var nin = dims(x)[0];
    var sigma = Math.sqrt(2 / (nin + nout));
    var g = nnparam({name: name + 'g', dims: [nout, 1], mu: 1, sigma: 0}); // gain
    var v = nnparam({name: name + 'v', dims: [nout, nin], sigma});
    var b = nnparam({name: name + 'b', dims: [nout, 1], sigma: 0});
    var norm = T.sqrt(webpplNn.sumreduce0(T.pow(v, 2)));
    return T.add(T.mul(T.div(g, norm), T.dot(v, x)), b);
  };
};
