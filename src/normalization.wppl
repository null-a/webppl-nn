// Center and rescale the input. Typical usage is to put this after a
// linear layer and follow it with a bias and non-linearity, yielding
// "Layer normalization".

// https://arxiv.org/abs/1607.06450

var layerNorm = function(name, maybeArgs) {
  checkNetName(name);
  var args = maybeArgs || {};
  var nnparam = args.param || param;
  var nout = args.out;
  assert.ok(Number.isInteger(nout), 'Argument "out" should be an integer.');
  // Gain parameters.
  var g = nnparam({name: name + 'g', dims: [nout, 1], mu: 1, sigma: 0});
  return function(a) {
    assert.ok(dims(a)[0] === nout,
              'Input vector has unexpected dimension.');
    var mu = T.sumreduce(a) / nout;
    var acentered = T.sub(a, mu);
    var sd = Math.sqrt(T.sumreduce(T.pow(acentered, 2)) / nout);
    assert.ok(ad.value(sd) > 0, 'Standard deviation of activations is not positive');
    return T.mul(T.div(g, sd), acentered);
  };
};

// TODO: You can imagine having a convenient way to do a restricted
// kind of composition of network constructors, where we assume the
// sub nets are stacked, and each take the same nout param. (Also
// requires more consistency in the interface of current constructors,
// e.g. we'd need to do something i like have a version of bias that
// takes a dummy nout param.) This could be useful for passing novel
// constructors to gru/lstm etc.

var affineWithLayerNorm = function(name, maybeArgs) {
  checkNetName(name);
  return stack([
    bias(name + 'b', maybeArgs),
    layerNorm(name + 'ln', maybeArgs),
    linear(name + 'w', maybeArgs)
  ]);
};

// Linear layer with weight normalization.
// https://arxiv.org/abs/1602.07868

// It's not clear how we could do the data dependent init., so we
// resort to Xavier initialization. Is this sensible? The problem is
// this might be used as part of a recurrent net (which is problematic
// for the reason mentioned in the paper) and because we don't have a
// straight-forward way of looking at a 'batch' from here.

// Does it make sense to apply regularization v here, given that we
// already normalize the length of the row vectors of v? In practice,
// regularization seems to have a much stronger effect on this
// parameterization. (You can see this with L2 prior by comparing this
// with the linear layer on the ml-classifier example.) Removing the
// prior from v seems to make a difference here.

// Note: When I tried parameterizing g in log space the ml-classifier
// example often failed to work well. I don't know why.

var linearWeightNorm = function(name, maybeArgs) {
  checkNetName(name);
  var args = maybeArgs || {};
  var nnparam = args.param || param;
  var nin = args.in;
  var nout = args.out;
  assert.ok(Number.isInteger(nin), 'Argument "in" should be an integer.');
  assert.ok(Number.isInteger(nout), 'Argument "out" should be an integer.');
  var sigma = Math.sqrt(2 / (nin + nout));
  var g = nnparam({name: name + 'g', dims: [nout, 1], mu: 1, sigma: 0}); // gain
  var v = nnparam({name: name + 'v', dims: [nout, nin], sigma});
  return function(x) {
    var norm = T.sqrt(webpplNn.sumreduce0(T.pow(v, 2)));
    return T.mul(T.div(g, norm), T.dot(v, x));
  };
};

var affineWeightNorm = function(name, maybeArgs) {
  checkNetName(name);
  return compose(bias(name + 'b', maybeArgs),
                 linearWeightNorm(name + 'wn', maybeArgs));
};
