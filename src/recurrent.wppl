// Traditional RNN

var rnn = function(name, maybeArgs) {
  checkNetName(name, true);
  var args = maybeArgs || {};
  var hdim = args.hdim;
  var xdim = args.xdim;
  assert.ok(Number.isInteger(hdim), 'Argument "hdim" should be an integer.');
  assert.ok(Number.isInteger(xdim), 'Argument "xdim" should be an integer.');
  var ctor = args.ctor || affine;
  var output = args.output || tanh;
  var nargs = mergeObj(args, {in: hdim + xdim, out: hdim});
  var net = stack([output, ctor(name, nargs), concat]);
  return function(hprev, x) {
    assert.ok(dims(hprev)[0] === hdim,
              'Previous hidden vector has unexpected dimension');
    assert.ok(dims(x)[0] === xdim,
              'Input vector has unexpected dimension');
    return net([hprev, x]);
  };
};

// Gated Recurrent Unit

// This is similar to the variant described in "Empirical Evaluation
// of Gated Recurrent Neural Networks on Sequence Modeling", which
// computes the candidate activation in a slightly different way from
// the original paper.

// https://arxiv.org/abs/1412.3555

var gru = function(name, maybeArgs) {
  checkNetName(name);
  var args = maybeArgs || {};
  var hdim = args.hdim;
  var xdim = args.xdim;
  assert.ok(Number.isInteger(hdim), 'Argument "hdim" should be an integer.');
  assert.ok(Number.isInteger(xdim), 'Argument "xdim" should be an integer.');
  var ctor = args.ctor || affine;
  var nargs = mergeObj(args, {in: hdim + xdim, out: hdim});
  var update = compose(sigmoid, ctor(name + 'update', nargs));
  var reset = compose(sigmoid, ctor(name + 'reset', nargs));
  var candidate = compose(tanh, ctor(name + 'candidate', nargs));
  return function(hprev, x) {
    assert.ok(dims(hprev)[0] === hdim,
              'Previous hidden vector has unexpected dimension');
    assert.ok(dims(x)[0] === xdim,
              'Input vector has unexpected dimension');
    var hprevx = concat([hprev, x]);
    var r = reset(hprevx);
    var z = update(hprevx);
    var cand = candidate(concat([T.mul(hprev, r), x]));
    var oneminusz = T.add(T.neg(z), 1);
    return T.add(T.mul(oneminusz, hprev), T.mul(z, cand));
  };
};

// Long Short Term Memory

// This is similar to the variant described in "Generating sequences
// with recurrent neural networks" (Graves 2013). The difference is
// that here there are no 'peep-hole' connections. i.e. The previous
// memory state is not (currently) passed as input to the forget,
// input, output gates.

// https://arxiv.org/abs/1308.0850

var lstm = function(name, maybeArgs) {
  checkNetName(name);
  var args = maybeArgs || {};
  var hdim = args.hdim;
  var xdim = args.xdim;
  // hdim is the total dimension of the state. i.e. memory + hidden
  // state vectors. Setting things up this way makes it easy to swap
  // between gru and lstm.
  assert.ok(Number.isInteger(hdim) && (hdim % 2 === 0),
            'Argument "hdim" should be an even integer.');
  assert.ok(Number.isInteger(xdim), 'Argument "xdim" should be an integer.');

  var dim = hdim / 2; // Dimension of memory and hidden state.
  var nargs = mergeObj(args, {in: dim + xdim, out: dim});
  // It's said that initializing the biases of the forget gate to a
  // value greater than 0 is a good idea. This is so that the output
  // is close to one at the start of optimization, ensuring
  // information is passed along. This is mentioned in e.g. "An
  // Empirical Exploration of Recurrent Network Architectures".
  var forget = compose(
    sigmoid,
    affine(name + 'forget', mergeObj(nargs, {initb: 1})));
  var input = compose(sigmoid, affine(name + 'input', nargs));
  var output = compose(sigmoid, affine(name + 'output', nargs));
  var candidate = compose(tanh, affine(name + 'candidate', nargs));
  return function(prev, x) {
    // For compatibility with the interface of e.g. gru we combine the
    // memory and hidden state into a single vector, prev.
    assert.ok(dims(prev)[0] === hdim,
              'Previous state vector has unexpected dimension');
    assert.ok(dims(x)[0] === xdim,
              'Input vector has unexpected dimension');
    var cprev = T.reshape(T.range(prev, 0, dim), [dim, 1]);
    var hprev = T.reshape(T.range(prev, dim, hdim), [dim, 1]);
    var hprevx = concat([hprev, x]);
    var f = forget(hprevx);
    var i = input(hprevx);
    var o = output(hprevx);
    var cand = candidate(hprevx);
    var c = T.add(T.mul(f, cprev), T.mul(i, cand));
    var h = T.mul(o, tanh(c));
    return concat([c, h]);
  };
};
